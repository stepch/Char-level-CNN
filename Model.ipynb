{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CharacterCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, text_length, conv_kernels, conv_dim, \n",
    "                 linear_dim, output_dim, init_weights, dropout_prob, pool_kernel):\n",
    "        \n",
    "        super(CharacterCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(vocab_size, conv_dim, conv_kernels[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = pool_kernel, stride = pool_kernel)# stride provides non-overlapping propagation\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(conv_dim, conv_dim, conv_kernels[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = pool_kernel, stride = pool_kernel)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(conv_dim, conv_dim, conv_kernels[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "            \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(conv_dim, conv_dim, conv_kernels[3]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(conv_dim, conv_dim, conv_kernels[4]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(conv_dim, conv_dim, conv_kernels[5]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = pool_kernel, stride = pool_kernel)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(int(conv_dim * (text_length - 96) / 27), linear_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(linear_dim, linear_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Linear(linear_dim, output_dim)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim = 0)\n",
    "        \n",
    "        self.weights_init(init_weights)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "# 6 convolutions with ReLU activations + 3 max poolings\n",
    "        output = self.conv1(input)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        output = self.conv4(output)\n",
    "        output = self.conv5(output)\n",
    "        output = self.conv6(output)\n",
    "        \n",
    "# flatten the input for the linear layer\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        \n",
    "# 3 affine maps\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "        \n",
    "# logsoftmax\n",
    "        output = self.logsoftmax(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# initialize weights with normal distribution\n",
    "    def weights_init(self, init_weights):\n",
    "        for m in self.modules():\n",
    "            if type(m) == nn.Linear:\n",
    "                m.weight.data.normal_(init_weights[0][0], init_weights[0][1])\n",
    "            elif type(m) == nn.Conv1d:\n",
    "                m.weight.data.normal_(init_weights[1][0], init_weights[1][1])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prepare data for DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path as op\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextLoader(Dataset):\n",
    "    def __init__(self, label_data_path, alphabet_path, text_length):\n",
    "\n",
    "        self.label_data_path = label_data_path\n",
    "        # read alphabet\n",
    "        with open(alphabet_path) as alphabet_file:\n",
    "            alphabet = str(''.join(json.load(alphabet_file)))\n",
    "        self.alphabet = alphabet\n",
    "        self.text_length = text_length\n",
    "        self.load()\n",
    "        self.y = torch.LongTensor(self.label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.oneHotEncode(idx)\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def load(self, lowercase=True):\n",
    "        self.label = []\n",
    "        self.data = []\n",
    "        with open(self.label_data_path, 'rt') as f:\n",
    "            rdr = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "            # num_samples = sum(1 for row in rdr)\n",
    "            for index, row in enumerate(rdr):\n",
    "                self.label.append(int(row[0]))\n",
    "                txt = ' '.join(row[1:])\n",
    "                if lowercase:\n",
    "                    txt = txt.lower()                \n",
    "                self.data.append(txt)\n",
    "\n",
    "    def oneHotEncode(self, idx):\n",
    "        X = torch.zeros(len(self.alphabet), self.text_length)\n",
    "        sequence = self.data[idx]\n",
    "        for index_char, char in enumerate(sequence[::-1]): # iterate over reversed sequence\n",
    "            if self.char2Index(char)!=-1:\n",
    "                X[self.char2Index(char)][index_char] = 1.0\n",
    "        return X\n",
    "\n",
    "    def char2Index(self, character):\n",
    "        return self.alphabet.find(character)\n",
    "\n",
    "    def get_class_weight(self):\n",
    "        \n",
    "        num_samples = self.__len__()\n",
    "        label_set = set(self.label)\n",
    "        num_class = [self.label.count(c) for c in label_set]\n",
    "        class_weight = [num_samples / float(self.label.count(c)) for c in label_set]    \n",
    "        \n",
    "        return class_weight, num_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        \n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "#         a = list(model.parameters())[0].clone()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm(model.parameters(), 400)\n",
    "        optimizer.step()\n",
    "#         b = list(model.parameters())[0].clone()\n",
    "#         print(torch.equal(a.data, b.data)) # checked that weights are updated\n",
    "    \n",
    "\n",
    "def evaluate(loader, model, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    avg_loss = 0\n",
    "    for batch_num, data in enumerate(loader):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = Variable(inputs)\n",
    "        \n",
    "        out = model(inputs)\n",
    "        \n",
    "        predicted = Variable(torch.max(out.data, 1)[1])\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.view(labels.size()).data == labels).sum()\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        labels = Variable(labels)\n",
    "        loss = criterion(out, labels)\n",
    "        avg_loss += (loss.data[0] - avg_loss) / (batch_num + 1)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def fit(model, optimizer, scheduler, criterion, train_loader, test_loader, n_epochs):\n",
    "\n",
    "    train_log, train_acc_log = [], []\n",
    "    val_log, val_acc_log = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        train(model, optimizer, train_loader, criterion)\n",
    "        train_loss, train_acc = evaluate(train_loader, model, criterion)\n",
    "        val_loss, val_acc = evaluate(test_loader, model, criterion)\n",
    "\n",
    "        train_log.append(train_loss)\n",
    "        train_acc_log.append(train_acc)\n",
    "\n",
    "        val_log.append(val_loss)\n",
    "        val_acc_log.append(val_acc)\n",
    "\n",
    "        print (('Epoch [%d/%d], LR: %f, Loss (train/test): %.4f/%.4f,'+\\\n",
    "               ' Acc (train/test): %.4f/%.4f' )\n",
    "                   %(epoch+1, n_epochs, \\\n",
    "                     optimizer.state_dict()['param_groups'][0]['lr'], train_loss, val_loss, train_acc, val_acc))\n",
    "            \n",
    "    return train_log, train_acc_log, val_log, val_acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], LR: 0.010000, Loss (train/test): 4.1279/4.1295, Acc (train/test): 19.7197/18.6000\n",
      "Epoch [2/10], LR: 0.010000, Loss (train/test): 4.1279/4.1295, Acc (train/test): 21.8218/20.8000\n",
      "Epoch [3/10], LR: 0.010000, Loss (train/test): 4.1279/4.1295, Acc (train/test): 23.0230/21.4000\n",
      "Epoch [4/10], LR: 0.001000, Loss (train/test): 4.1279/4.1295, Acc (train/test): 23.4234/20.1000\n",
      "Epoch [5/10], LR: 0.001000, Loss (train/test): 4.1279/4.1295, Acc (train/test): 23.5235/20.9000\n",
      "Epoch [6/10], LR: 0.001000, Loss (train/test): 4.1279/4.1295, Acc (train/test): 23.5235/21.0000\n",
      "Epoch [7/10], LR: 0.000100, Loss (train/test): 4.1279/4.1295, Acc (train/test): 23.7237/20.8000\n",
      "Epoch [8/10], LR: 0.000100, Loss (train/test): 4.1279/4.1295, Acc (train/test): 23.9239/20.7000\n",
      "Epoch [9/10], LR: 0.000100, Loss (train/test): 4.1279/4.1295, Acc (train/test): 24.0240/20.6000\n",
      "Epoch [10/10], LR: 0.000010, Loss (train/test): 4.1279/4.1295, Acc (train/test): 24.0240/20.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4.127929776906967,\n",
       "  4.1279273480176935,\n",
       "  4.1279269605875015,\n",
       "  4.12792693078518,\n",
       "  4.127926513552666,\n",
       "  4.127926662564277,\n",
       "  4.1279266923666,\n",
       "  4.127926751971245,\n",
       "  4.127926692366599,\n",
       "  4.1279266923666],\n",
       " [19.71971971971972,\n",
       "  21.82182182182182,\n",
       "  23.023023023023022,\n",
       "  23.423423423423422,\n",
       "  23.523523523523522,\n",
       "  23.523523523523522,\n",
       "  23.723723723723722,\n",
       "  23.923923923923923,\n",
       "  24.024024024024023,\n",
       "  24.024024024024023],\n",
       " [4.129515290260315,\n",
       "  4.129512429237366,\n",
       "  4.129512235522271,\n",
       "  4.129511386156082,\n",
       "  4.12951086461544,\n",
       "  4.12951135635376,\n",
       "  4.129511207342148,\n",
       "  4.129511207342148,\n",
       "  4.129511147737503,\n",
       "  4.129511207342148],\n",
       " [18.6, 20.8, 21.4, 20.1, 20.9, 21.0, 20.8, 20.7, 20.6, 20.6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Define model hyperparameters\n",
    "VOCAB_SIZE = 70\n",
    "TEXT_LENGTH = 1014\n",
    "CONV_KERNELS = [7, 7, 3, 3, 3, 3]\n",
    "POOL_KERNEL = 3\n",
    "CONV_DIM = 256\n",
    "LINEAR_DIM = 1024\n",
    "INIT_WEIGHTS = [[0, 0.02], [0, 0.02]]\n",
    "DROPOUT_PROB = 0.5\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "# Define training parameters\n",
    "EPOCHS_NUM = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "alphabet_path = 'data/alphabet.json'\n",
    "train_data_path = 'data/train_1k.csv'\n",
    "test_data_path = 'data/test_1k.csv'\n",
    "              \n",
    "loss_func = nn.NLLLoss()\n",
    "# loss_func = nn.CrossEntropy()\n",
    "model = CharacterCNN(VOCAB_SIZE, TEXT_LENGTH, CONV_KERNELS, CONV_DIM, \n",
    "                     LINEAR_DIM, OUTPUT_DIM, INIT_WEIGHTS, DROPOUT_PROB, POOL_KERNEL)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # halve learning rate 10 times every 3 epochs\n",
    "\n",
    "train_dataset = TextLoader(train_data_path, alphabet_path, TEXT_LENGTH)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
    "\n",
    "test_dataset = TextLoader(test_data_path, alphabet_path, TEXT_LENGTH)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE)\n",
    "\n",
    "# Model fit\n",
    "fit(model, optimizer, scheduler, loss_func, train_loader, test_loader, EPOCHS_NUM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
